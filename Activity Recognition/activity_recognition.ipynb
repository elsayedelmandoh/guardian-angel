{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardian Angel - Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Kid Level Activity Recognition\n",
    "\n",
    "+ Objective: \n",
    "    - Develop a system that identifies `specific activities` performed by individual children in video frames, \n",
    "    - categorizing them into predefined activities such as `Playing`, `Learning`, and `Violence`.\n",
    "\n",
    "+ Technologies and Methods:\n",
    "\n",
    "    - Convolutional Neural Networks (CNNs): \n",
    "        - Use CNNs to analyze visual data from video frames for robust feature extraction specific to various activities.\n",
    "\n",
    "    - Action Recognition Models:\n",
    "        - Implement action recognition models that can distinguish between different types of activities `based on movement patterns and context`.\n",
    "\n",
    "    - Temporal Segmentation Networks (TSNs): \n",
    "        - `Utilize TSNs` to capture long-range temporal structures in the video data, enhancing the ability to recognize prolonged activities.\n",
    "\n",
    "    - Transfer Learning: \n",
    "        - `Employ pre-trained` models on large datasets and `fine-tune them` to the specific task of recognizing child activities to `improve accuracy and reduce training time`.\n",
    "\n",
    "    - Pose Estimation: \n",
    "        - `Apply pose estimation` techniques to `understand body positions and movements` that are indicative of different activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Scene Level Activity Recognition for All Kids\n",
    "\n",
    "+ Objective: \n",
    "    - Create a system capable of `recognizing and analyzing activities` involving `multiple children within a scene`, providing an overview of the collective activity in the environment.\n",
    "\n",
    "+ Technologies and Methods:\n",
    "    \n",
    "    - Scene Recognition Algorithms: \n",
    "        - Use scene recognition techniques to understand the broader context of the environment which helps in interpreting group activities.\n",
    "\n",
    "    - Graph Neural Networks (GNNs): \n",
    "        - Implement GNNs to model interactions between multiple individuals in a scene, which is vital for understanding collective activities.\n",
    "\n",
    "    - Multiple Object Tracking (MOT): \n",
    "        - Employ MOT systems to track multiple children simultaneously, ensuring accurate activity recognition even in dynamic scenes.\n",
    "\n",
    "    - Deep Learning for Video Classification: \n",
    "        - Utilize deep learning models designed for video classification to analyze and classify complex activities involving multiple participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my supervior said we not have many images for training but we have same videos for training but we make testing we only will have one image for kid and caregiver so how handel this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extract Frames from the Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, output_folder, frame_interval):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate of the video\n",
    "    frame_count = 0\n",
    "    extracted_count = 0\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    while success:\n",
    "        if frame_count % (fps * frame_interval) < 1: # one frame per second is extracted accurately\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "            extracted_count += 1\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {extracted_count} frames from the video: {video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 0 frames from the video: ..\\datasets\\videos\\Learning.mp4\n",
      "Extracted 0 frames from the video: ..\\datasets\\videos\\Playing.mp4\n"
     ]
    }
   ],
   "source": [
    "video_learning = r'..\\datasets\\videos\\Learning.mp4'\n",
    "video_playing = r'..\\datasets\\videos\\Playing.mp4'\n",
    "\n",
    "output_folder_learning = r'..\\datasets\\videos\\Learning_frames'\n",
    "output_folder_playing = r'..\\datasets\\videos\\Playing_frames'\n",
    "\n",
    "frame_interval = 1  # Extract one frame every 1 second\n",
    "\n",
    "extract_frames(video_learning, output_folder_learning, frame_interval)\n",
    "extract_frames(video_playing, output_folder_playing, frame_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Face Detection and Annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(image, face_detector):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return faces\n",
    "\n",
    "face_detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "< cv2.CascadeClassifier 00000285655A9D50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detected_faces(frame_folder, output_kids, output_caregivers):\n",
    "    if not os.path.exists(output_kids):\n",
    "        os.makedirs(output_kids)\n",
    "    if not os.path.exists(output_caregivers):\n",
    "        os.makedirs(output_caregivers)\n",
    "\n",
    "    frame_files = [os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith('.jpg')]\n",
    "    for frame_file in frame_files:\n",
    "        image = cv2.imread(frame_file)\n",
    "        faces = detect_faces(image, face_detector)\n",
    "        for i, (x, y, w, h) in enumerate(faces):\n",
    "            face = image[y:y+h, x:x+w]\n",
    "            face_filename = os.path.join(output_kids, f\"{os.path.basename(frame_file).split('.')[0]}_face_{i}.jpg\")\n",
    "            cv2.imwrite(face_filename, face)\n",
    "    print(f\"Saved detected faces to {output_kids}\")\n",
    "\n",
    "# Define paths\n",
    "kids_frame_folder = r'Learning_frames'\n",
    "caregiver_frame_folder = r'Playing_frames'\n",
    "\n",
    "output_kids = r'\\Kids_faces'\n",
    "output_caregivers = r'\\Caregiver_faces'\n",
    "\n",
    "# Extract faces\n",
    "save_detected_faces(kids_frame_folder, output_kids, output_caregivers, 'kid')\n",
    "save_detected_faces(caregiver_frame_folder, output_kids, output_caregivers, 'caregiver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, target_size):\n",
    "    image = load_img(image_path, target_size=target_size)\n",
    "    image = img_to_array(image)\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "def preprocess_frames(frame_folder, target_size):\n",
    "    processed_images = []\n",
    "    frame_files = [os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith('.jpg')]\n",
    "    for frame_file in frame_files:\n",
    "        processed_image = preprocess_image(frame_file, target_size)\n",
    "        processed_images.append(processed_image)\n",
    "    return np.array(processed_images)\n",
    "\n",
    "target_size = (224, 224)  # Example target size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 85 Learning frames.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Learning frames\n",
    "learning_frame_folder = r'..\\datasets\\videos\\Learning_frames'\n",
    "learning_images = preprocess_frames(learning_frame_folder, target_size)\n",
    "print(f\"Preprocessed {len(learning_images)} Learning frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 733 Playing frames.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Playing frames\n",
    "playing_frame_folder = r'..\\datasets\\videos\\Playing_frames'\n",
    "playing_images = preprocess_frames(playing_frame_folder, target_size)\n",
    "print(f\"Preprocessed {len(playing_images)} Playing frames.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Labels with Preprocessed Images\n",
    "\n",
    "Assume labels for learning and playing are 0 and 1 respectively. We create the combined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 818\n",
      "Total labels: 818\n"
     ]
    }
   ],
   "source": [
    "learning_labels = np.zeros(len(learning_images))  # Label for Learning is 0\n",
    "playing_labels = np.ones(len(playing_images))    # Label for Playing is 1\n",
    "\n",
    "# Combine images and labels\n",
    "all_images = np.concatenate((learning_images, playing_images), axis=0)\n",
    "all_labels = np.concatenate((learning_labels, playing_labels), axis=0)\n",
    "\n",
    "print(f\"Total images: {all_images.shape[0]}\")\n",
    "print(f\"Total labels: {all_labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 654 samples\n",
      "Testing set: 164 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Fine-Tune the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 244s 11s/step - loss: 0.0584 - accuracy: 0.9572 - val_loss: 0.4167 - val_accuracy: 0.8659\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 170s 8s/step - loss: 2.8287e-05 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.8659\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 191s 9s/step - loss: 1.6789e-05 - accuracy: 1.0000 - val_loss: 0.4511 - val_accuracy: 0.8659\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 165s 8s/step - loss: 1.9942e-05 - accuracy: 1.0000 - val_loss: 0.7336 - val_accuracy: 0.8659\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 165s 8s/step - loss: 1.4293e-05 - accuracy: 1.0000 - val_loss: 1.2246 - val_accuracy: 0.8659\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 163s 8s/step - loss: 7.4046e-06 - accuracy: 1.0000 - val_loss: 1.8389 - val_accuracy: 0.8659\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 170s 8s/step - loss: 4.4442e-06 - accuracy: 1.0000 - val_loss: 2.6011 - val_accuracy: 0.8659\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 172s 8s/step - loss: 7.5229e-06 - accuracy: 1.0000 - val_loss: 3.3962 - val_accuracy: 0.8659\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 167s 8s/step - loss: 5.9077e-06 - accuracy: 1.0000 - val_loss: 4.1045 - val_accuracy: 0.8659\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 158s 7s/step - loss: 4.1910e-06 - accuracy: 1.0000 - val_loss: 4.7180 - val_accuracy: 0.8659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e8979174d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_fine_tuned_model(base_model, num_classes):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Convert labels to categorical one-hot encoding\n",
    "y_train_cat = to_categorical(y_train, num_classes=2)\n",
    "y_test_cat = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "model = build_fine_tuned_model(base_model, num_classes=2)  # 2 classes: Playing, Learning\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train_cat, epochs=10, batch_size=32, validation_data=(X_test, y_test_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 11s 2s/step - loss: 4.7180 - accuracy: 0.8659\n",
      "Test accuracy: 86.59%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Frame Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "def single_frame_prediction(image_path, model, target_size):\n",
    "    image = preprocess_image(image_path, target_size)\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    prediction = model.predict(image)\n",
    "    return prediction\n",
    "\n",
    "# Example usage:\n",
    "single_frame = r'..\\datasets\\videos\\Learning_frames\\frame_0.jpg'\n",
    "prediction = single_frame_prediction(single_frame, model, target_size)\n",
    "print(f\"Prediction: {np.argmax(prediction)}\")  # 0 for Learning, 1 for Playing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
